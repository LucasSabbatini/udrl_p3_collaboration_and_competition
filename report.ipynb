{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Tennis with Proximal Policy Optimization (PPO)\n",
    "\n",
    "## Collaboration and Competition: Multi-Agent RL\n",
    "\n",
    "Multi-Agent Reinforcement Learning (MARL) involves multiple agents learning to interact within a shared environment. It's used in diverse areas like robotics, autonomous vehicles, and gaming. Key challenges in MARL include handling non-stationarity, credit assignment among agents, effective communication, and scalability.\n",
    "\n",
    "Proximal Policy Optimization (PPO) is often chosen for MARL due to its simplicity, sample efficiency, and balance in exploration and exploitation. PPO is adaptable for both cooperative and competitive multi-agent scenarios, making it suitable for environments like the Unity Tennis environment, where agents can learn collaboratively. Its robustness and stable learning process help manage the complexities inherent in MARL.\n",
    "\n",
    "## Multi-agent Proximal Policy Optimization (MAPPO)\n",
    "\n",
    "As mentioned above, Proximal Policy Optimization (PPO) is a widely used reinforcement learning algorithm known for its stability and effectiveness, especially in environments with continuous action spaces. PPO belongs to the family of policy gradient methods, offering a balance between sample efficiency and ease of implementation.\n",
    "\n",
    "### **Multi-Agent Scenario:**\n",
    "\n",
    "In a multi-agent scenario, like the Tennis environment from Unity, PPO can be applied with some considerations:\n",
    "\n",
    "1. **Independent Observations**: Each agent receives its own observation from the environment. In the Tennis environment, observations are tailored for each agent, meaning that each agent's perspective is already considered. This setup simplifies the process as there's no need for additional modifications to handle different perspectives.\n",
    "2. **Rewards Relative to Agent**: Each agent receives rewards relative to its actions and state, independent of the other agents. This is ideal for the Tennis environment, where each agent's reward is calculated based on its performance, and not directly on the other agent's actions. This approach aligns well with PPO's objective of maximizing the expected return.\n",
    "\n",
    "\n",
    "All this means that we can use one policy and one critic network for both agents, and train them as if it was a batch with multiple agents (2 in this case) playing separate gammes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the Agent Architecture\n",
    "\n",
    "In the realm of Multi-Agent Reinforcement Learning (MARL), the provided Python program delineates an intricate agent architecture tailored for environments where multiple agents interact, learn, and make decisions concurrently. This architecture is underpinned by the Actor-Critic model, a staple in reinforcement learning, which is augmented for handling multiple agents efficiently.\n",
    "\n",
    "### The Actor-Critic Model\n",
    "\n",
    "1. **Actor Network**: The Actor network, a crucial component of the architecture, is responsible for generating actions given the current state. It consists of three fully connected layers. The first two layers apply the ReLU activation function for non-linearity, while the final layer employs the tanh function to output action values within a specific range, usually [-1, 1]. This structure ensures a balance between expressiveness and computational efficiency.\n",
    "2. **Critic Network**: Parallel to the Actor, the Critic network estimates the value of the current state, guiding the Actor's decisions. It mirrors the Actor's structure with three fully connected layers and ReLU activations, outputting a scalar value representing the state's value.\n",
    "3. **ActorCritic Class**: This class unifies the Actor and Critic networks, creating a cohesive model that outputs both action distributions (from the Actor) and state values (from the Critic). The standard deviation for the action distribution is also adjustable, providing a knob for tuning the exploration-exploitation trade-off.\n",
    "\n",
    "### The Agent Class\n",
    "\n",
    "1. **Initialization**: The Agent class initializes with the number of agents, state and action sizes, and various hyperparameters like learning rate, optimizer epsilon, weight decay, and more. It creates an instance of the ActorCritic model and sets up an optimizer (Adam in this case) for learning.\n",
    "2. **Acting and Learning**:\n",
    "    - The **`act`** method enables the agent to select actions based on the current state, using the ActorCritic model. It returns the chosen actions, their log probabilities, and the corresponding state values.\n",
    "    - The **`learn`** method facilitates the agent's learning from experiences. It uses batches of experiences to update the model's weights using the Proximal Policy Optimization (PPO) algorithm, which involves several epochs of mini-batch updates.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Multi-Agent Compatibility**: This architecture is adept at handling scenarios involving multiple agents, a fundamental aspect of complex environments like the Unity Tennis environment.\n",
    "- **Flexibility and Scalability**: The use of neural networks offers flexibility in learning various tasks, and the parameterization allows scalability to different problem sizes and complexities.\n",
    "- **Balance of Exploration and Exploitation**: The use of a stochastic policy (through the normal distribution in action selection) allows for both exploration of the state space and exploitation of learned policies.\n",
    "\n",
    "In summary, this agent architecture is a comprehensive, adaptable framework for tackling complex MARL problems, leveraging the strengths of the Actor-Critic approach and PPO algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent and models\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# from PPO import ppo_loss\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x)) # still the same, since the range is [-1.0, +1.0]\n",
    "    \n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, value_size=1, hidden_size=64):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, value_size)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, value_size=1, hidden_size=64, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = Actor(state_size, action_size, hidden_size)\n",
    "        self.critic = Critic(state_size, value_size, hidden_size)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_size)*std)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        obs = torch.FloatTensor(states)\n",
    "        \n",
    "        # Critic\n",
    "        values = self.critic(obs)\n",
    "        \n",
    "        # Actor\n",
    "        mu = self.actor(obs)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        \n",
    "        return dist, values\n",
    "    \n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, num_agents, state_size, action_size,\n",
    "                 LR=3.e4,\n",
    "                 op_epsilon=1.e-5,\n",
    "                 weight_decay=1.e-4,\n",
    "                 batch_size=32,\n",
    "                 sgd_epochs=4,\n",
    "                 gradient_clip=5,\n",
    "                 std=0.1,\n",
    "                 value_size=1,\n",
    "                 hidden_size=64,\n",
    "                 clip_epsilon=0.1,\n",
    "                 c1=0.5,\n",
    "                 beta=0.01):\n",
    "        \n",
    "        self.num_agents = num_agents\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.lr = LR\n",
    "        self.model = ActorCritic(state_size, action_size, value_size=value_size, hidden_size=hidden_size, std=std)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR, weight_decay=weight_decay, eps=op_epsilon)\n",
    "        # self.optimizer = optim.Adam(self.model.parameters(), lr=LR, eps=op_epsilon)\n",
    "        \n",
    "        self.sgd_epochs = sgd_epochs\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.c1 = c1\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "    def act(self, states):\n",
    "        \"\"\"Remember: states are state vectors for each agent\n",
    "        It is used when collecting trajectories\n",
    "        \"\"\"\n",
    "        dist, values = self.model(states) # pass the state trough the network and get a distribution over actions and the value of the state\n",
    "        actions = dist.sample() # sample an action from the distribution\n",
    "        log_probs = dist.log_prob(actions) # calculate the log probability of that action\n",
    "        log_probs = log_probs.sum(-1).unsqueeze(-1) # sum the log probabilities of all actions taken (in case of multiple actions) and reshape to (batch_size, 1)\n",
    "        \n",
    "        return actions, log_probs, values\n",
    "    \n",
    "    def learn(self, states, actions, log_probs_old, returns, advantages, sgd_epochs=4):\n",
    "        \"\"\" Performs a learning step given a batch of experiences\n",
    "        \n",
    "        Remmeber: in the PPO algorithm, we perform SGD_episodes (usually 4) weights update steps per batch\n",
    "        using the proximal policy ratio clipped objective function\n",
    "        \"\"\"        \n",
    "\n",
    "        num_batches = states.size(0) // self.batch_size\n",
    "        for i in range(self.sgd_epochs):\n",
    "            batch_count = 0\n",
    "            batch_ind = 0\n",
    "            for i in range(num_batches):\n",
    "                sampled_states = states[batch_ind:batch_ind+self.batch_size, :]\n",
    "                sampled_actions = actions[batch_ind:batch_ind+self.batch_size, :]\n",
    "                sampled_log_probs_old = log_probs_old[batch_ind:batch_ind+self.batch_size, :]\n",
    "                sampled_returns = returns[batch_ind:batch_ind+self.batch_size, :]\n",
    "                sampled_advantages = advantages[batch_ind:batch_ind+self.batch_size, :]\n",
    "                \n",
    "                L = ppo_loss(self.model, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages,\n",
    "                             clip_epsilon=self.clip_epsilon, c1=self.c1, beta=self.beta)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                (L).backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                batch_ind += self.batch_size\n",
    "                batch_count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Loss\n",
    "\n",
    "PPO is a policy gradient method that seeks to update policies in a way that avoids large and potentially harmful policy updates. Let's break down the key components of this function:\n",
    "\n",
    "1. **Probability Ratios**: The function starts by calculating the probability ratio **`r(θ) = π(a|s) / π_old(a|s)`**, where **`π(a|s)`** is the probability of taking action **`a`** in state **`s`** under the current policy, and **`π_old(a|s)`** is the probability under the old policy. This is computed by exponentiating the difference between the new and old log probabilities of the actions.\n",
    "2. **Advantages**: The **`advantages`** parameter represents the advantage estimates A(s,a), which indicate how much better an action is compared to the average. They are used to scale the policy ratio.\n",
    "3. **Objective Function**: The objective function in PPO, **`L_CPI(θ) = r(θ) * A`**, is a product of the probability ratio and the advantages. This function encourages the policy to increase the probability of actions that lead to higher than expected returns.\n",
    "4. **Clipping**: To avoid large policy updates, PPO introduces a clipping mechanism. The policy ratio is clipped within the range **`[1 - ε, 1 + ε]`**, where ε is a small value (**`clip_epsilon`** in this code). The final objective is the minimum of the unclipped and clipped objectives.\n",
    "5. **Entropy Regularization**: Entropy regularization (**`beta * entropy.mean()`**) is added to the loss to encourage exploration by discouraging the policy from becoming too deterministic.\n",
    "6. **Value Loss**: The value loss **`L_VF(θ) = (V(s) - V_t)^2`** is a mean squared error term between the predicted state values (**`values`**) and the returns. This encourages the critic to accurately predict the expected returns. The coefficient **`c1`** adjusts the importance of this term in the total loss.\n",
    "7. **Total Loss**: The total loss is the sum of the policy loss and the value loss. The policy loss is computed as the negative of the minimum between the clipped and unclipped objectives minus the entropy regularization term. The negative sign ensures that maximizing the objective function corresponds to minimizing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Loss function. NOT INTEGRATED YET\n",
    "def ppo_loss(model, states, actions, log_probs_old, returns, advantages, clip_epsilon=0.1, c1=0.5, beta=0.01):\n",
    "    dist, values = model(states)\n",
    "    \n",
    "    log_probs = dist.log_prob(actions)\n",
    "    log_probs = torch.sum(log_probs, dim=1, keepdim=True)\n",
    "    entropy = dist.entropy().mean()\n",
    "    \n",
    "    # r(θ) =  π(a|s) / π_old(a|s)\n",
    "    ratio = (log_probs - log_probs_old).exp() # NOTE WHYYYYYYY????\n",
    "    \n",
    "    # Surrogate Objctive : L_CPI(θ) = r(θ) * A\n",
    "    obj = ratio * advantages \n",
    "    \n",
    "    # clip ( r(θ), 1-Ɛ, 1+Ɛ )*A\n",
    "    obj_clipped = ratio.clamp(1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages\n",
    "    \n",
    "    # L_CLIP(θ) = E { min[ r(θ)A, clip ( r(θ), 1-Ɛ, 1+Ɛ )*A ] - β * KL }\n",
    "    policy_loss = -torch.min(obj, obj_clipped).mean(0) - beta * entropy.mean() # NOTE: WHY ARE WE TAKING THE MEAN AGAIN???\n",
    "    \n",
    "    # L_VF(θ) = ( V(s) - V_t )^2\n",
    "    value_loss = c1 * (returns - values).pow(2).mean()\n",
    "    \n",
    "    return policy_loss + value_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Advantage Estimation\n",
    "\n",
    "The **`calculate_advantages`** function in this code calculates the Generalized Advantage Estimation, and is designed to evaluate how beneficial each action taken by the agent is, compared to an average action, in the context of a reinforcement learning environment. It processes a series of states, actions, and rewards (referred to as a \"rollout\") and performs the following steps:\n",
    "\n",
    "1. **Temporal Difference (TD) Error Calculation**: For each state in the rollout, it calculates the TD error, which is the difference between the predicted value of the current state and the combined reward and predicted value of the next state.\n",
    "2. **Advantage Estimation**: Using the TD errors, the function computes the 'advantage' for each state. The advantage is a measure of how much better an action is compared to the average action at a given state. It uses Generalized Advantage Estimation (GAE) for a balance between bias and variance in these estimates.\n",
    "3. **Normalization**: The advantages are normalized (adjusted to have a mean of zero and a unit standard deviation) to improve learning stability.\n",
    "4. **Return Processed Data**: Finally, the function returns the processed data (states, actions, and their respective advantages), which are used to update the agent's policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(rollout, returns, num_agents, gamma=0.99, tau=0.95):\n",
    "    \"\"\" Given a rollout, calculates the advantages for each state \"\"\"\n",
    "    num_steps = len(rollout) - 1\n",
    "    processed_rollout = [None] * num_steps\n",
    "    advantages = torch.zeros((num_agents, 1))\n",
    "\n",
    "    for i in reversed(range(num_steps)):\n",
    "        states, value, actions, log_probs, rewards, dones = map(lambda x: torch.Tensor(x), rollout[i])\n",
    "        next_value = rollout[i + 1][1]\n",
    "\n",
    "        dones = dones.unsqueeze(1)\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "\n",
    "        # Compute the updated returns\n",
    "        returns = rewards + gamma * dones * returns\n",
    "\n",
    "        # Compute temporal difference error\n",
    "        td_error = rewards + gamma * dones * next_value.detach() - value.detach()\n",
    "        \n",
    "        advantages = advantages * tau * gamma * dones + td_error\n",
    "        processed_rollout[i] = [states, actions, log_probs, returns, advantages]\n",
    "\n",
    "    # Concatenate along the appropriate dimension\n",
    "    states, actions, log_probs_old, returns, advantages = map(lambda x: torch.cat(x, dim=0), zip(*processed_rollout))\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    return states, actions, log_probs_old, returns, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "from src.utils import test_agent\n",
    "import os\n",
    "from PPO import calculate_advantages\n",
    "from agent import Agent\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def collect_trajectories(env, brain_name, agent, max_t):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    num_agents = len(env_info.agents)\n",
    "    states = env_info.vector_observations\n",
    "        \n",
    "    rollout = []\n",
    "    agents_rewards = np.zeros(num_agents)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for s in range(max_t):\n",
    "    # s = 0\n",
    "    # while True:\n",
    "        actions, log_probs, values = agent.act(states)\n",
    "        env_info = env.step(actions.cpu().detach().numpy())[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards \n",
    "        dones = np.array([1 if t else 0 for t in env_info.local_done])\n",
    "        agents_rewards += rewards\n",
    "\n",
    "        for j, done in enumerate(dones):\n",
    "            if dones[j]:\n",
    "                episode_rewards.append(agents_rewards[j])\n",
    "                agents_rewards[j] = 0\n",
    "\n",
    "        rollout.append([states, values.detach(), actions.detach(), log_probs.detach(), rewards, 1 - dones])\n",
    "        # s += 1\n",
    "        states = next_states\n",
    "        # if np.any(dones):                                        # exit loop if episode finished\n",
    "        #     l_s = s\n",
    "        \n",
    "    # print(f\"Last step: {l_s}.\")\n",
    "\n",
    "    pending_value = agent.model(states)[-1]\n",
    "    returns = pending_value.detach() \n",
    "    rollout.append([states, pending_value, None, None, None, None])\n",
    "    \n",
    "    return rollout, returns, episode_rewards, np.mean(episode_rewards)\n",
    "\n",
    "\n",
    "def train(env, brain_name, agent, num_agents, n_episodes, max_t, gamma=0.99, tau=0.95, run_name=\"testing_02\", save_path=\"..\"):\n",
    "    print(f\"Starting training...\")\n",
    "    time.sleep(2)\n",
    "    env.info = env.reset(train_mode = True)[brain_name]\n",
    "    all_scores = []\n",
    "    all_scores_window = deque(maxlen=100)\n",
    "    best_so_far = 0.0\n",
    "    best_so_far_ep = 0\n",
    "    for i_episode in range(n_episodes):\n",
    "        # Each iteration, N parallel actors collect T time steps of data\n",
    "        rollout, returns, _, _ = collect_trajectories(env, brain_name, agent, max_t)\n",
    "        \n",
    "        states, actions, log_probs_old, returns, advantages = calculate_advantages(rollout, returns, num_agents, gamma=gamma, tau=tau)\n",
    "        # print(f\"States: {states.shape}. Actions: {actions.shape}. Log_probs_old: {log_probs_old.shape}. Returns: {returns.shape}. Advantages: {advantages.shape}\")\n",
    "        agent.learn(states, actions, log_probs_old, returns, advantages)\n",
    "        \n",
    "        test_mean_reward = test_agent(env, agent, brain_name)\n",
    "\n",
    "        all_scores.append(test_mean_reward)\n",
    "        all_scores_window.append(test_mean_reward)      \n",
    "        \n",
    "        if (i_episode + 1) % 20 == 0:\n",
    "            \n",
    "            if np.mean(all_scores_window) > best_so_far:\n",
    "                \n",
    "                if not os.path.isdir(f\"{save_path}/ckpt/{run_name}/\"):\n",
    "                    os.mkdir(f\"{save_path}/ckpt/{run_name}/\")\n",
    "                torch.save(agent.model.state_dict(), f\"{save_path}/ckpt/{run_name}/ep_{i_episode}_avg_score_{np.mean(all_scores_window)}.ckpt\")\n",
    "                best_so_far_ep = i_episode\n",
    "                best_so_far = np.mean(all_scores_window)\n",
    "                print(f\"Saved checkpoint with average score {np.mean(all_scores_window)}\")\n",
    "                if np.mean(all_scores_window) > 30:        \n",
    "                    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(all_scores_window)))\n",
    "                    # break       \n",
    "            elif i_episode - best_so_far_ep > 100:\n",
    "                print(f\"Early stopping. Best average score so far: {best_so_far}\")\n",
    "                break\n",
    "            \n",
    "            print('Episode {}, Total score this episode: {}, Last {} average: {}'.format(i_episode + 1, test_mean_reward, min(i_episode + 1, 100), np.mean(all_scores_window)) )\n",
    "        \n",
    "    save_scores(all_scores, run_name, save_path)\n",
    "    return all_scores\n",
    "\n",
    "def save_scores(scores, run_name, save_path):\n",
    "    print(f\"Saving scores to {save_path}/ckpt/{run_name}/scores.npy\")\n",
    "    if not os.path.isdir(f\"{save_path}/ckpt/{run_name}/\"):\n",
    "        os.mkdir(f\"{save_path}/ckpt/{run_name}\")\n",
    "    np.save(f\"{save_path}/ckpt/{run_name}/scores.npy\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters:\n",
    "\n",
    "1. **EPISODES**: This is the number of training episodes. The ideal number depends on the complexity of the environment and the learning capacity of the model. In complex environments, a higher number might be necessary for adequate learning.\n",
    "2. **MAX_T**: This represents the maximum length of each trajectory (or episode). Setting this depends on the average length of an episode in your environment and how long you expect the agent to learn meaningful actions within an episode.\n",
    "3. **SGD_EPOCHS**: This is the number of times the model will update its weights per batch of experiences. More epochs could lead to better learning but can also increase the risk of overfitting. A balance is needed based on the complexity of the task.\n",
    "4. **BATCH_SIZE**: This determines how many experiences are used to update the model at once. Larger batch sizes can lead to more stable, but potentially slower, learning. Smaller batches can speed up training but might lead to less stable learning.\n",
    "5. **BETA**: This parameter controls the strength of entropy regularization in the loss function. Entropy regularization encourages exploration by discouraging the policy from being too deterministic. Adjust based on how much exploration is needed.\n",
    "6. **GRADIENT_CLIP**: This parameter limits the value of gradients to a maximum (and minimum) to prevent exploding gradients, a common problem in training deep neural networks. The optimal value depends on the architecture and scale of gradients in your specific problem.\n",
    "7. **LR**: Learning rate for the optimizer. It determines how much the model weights are updated during training. A smaller learning rate ensures more stable updates, but training might be slower. A larger rate speeds up training but might overshoot the optimal values.\n",
    "8. **OP_EPSILON**: Epsilon value for the optimizer, providing numerical stability. This prevents division by zero errors during optimization.\n",
    "9. **WEIGHT_DECAY**: This is the L2 penalty (regularization term) parameter. It helps prevent the model from overfitting by penalizing large weights.\n",
    "10. **GAMMA**: Discount factor for future rewards. A higher value places more importance on future rewards, while a lower value makes the agent prioritize immediate rewards.\n",
    "11. **TAU**: This is the GAE (Generalized Advantage Estimation) parameter, balancing bias and variance in the advantage function estimation.\n",
    "12. **PPO_CLIP_EPSILON**: This is the clipping parameter in PPO's objective function. It prevents the policy from changing too drastically from one update to the next, thus ensuring stable learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2. State size: 24. Action size: 2\n",
      "Starting training with parameters LR=0.0003, WEIGHT_DECAY=0.0001, BATCH_SIZE=32, SGD_EPOCHS=4, GRADIENT_CLIP=5, BETA=0.01, GAMMA=0.99, TAU=0.95, PPO_CLIP_EPSILON=0.2\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with average score 0.027250000601634385\n",
      "Episode 20, Total score this episode: -0.004999999888241291, Last 20 average: 0.027250000601634385\n",
      "Episode 40, Total score this episode: 0.04500000085681677, Last 40 average: 0.02362500054296106\n",
      "Episode 60, Total score this episode: -0.004999999888241291, Last 60 average: 0.02408333388157189\n",
      "Saved checkpoint with average score 0.03056250064400956\n",
      "Episode 80, Total score this episode: -0.004999999888241291, Last 80 average: 0.03056250064400956\n",
      "Episode 100, Total score this episode: -0.004999999888241291, Last 100 average: 0.028950000619515776\n",
      "Episode 120, Total score this episode: -0.004999999888241291, Last 100 average: 0.026000000573694705\n",
      "Episode 140, Total score this episode: -0.004999999888241291, Last 100 average: 0.025000000558793544\n",
      "Episode 160, Total score this episode: 0.04500000085681677, Last 100 average: 0.025500000566244126\n",
      "Episode 180, Total score this episode: -0.004999999888241291, Last 100 average: 0.02900000061839819\n",
      "Saved checkpoint with average score 0.036000000722706316\n",
      "Episode 200, Total score this episode: -0.004999999888241291, Last 100 average: 0.036000000722706316\n",
      "Saved checkpoint with average score 0.044000000841915604\n",
      "Episode 220, Total score this episode: -0.004999999888241291, Last 100 average: 0.044000000841915604\n",
      "Saved checkpoint with average score 0.048000000901520255\n",
      "Episode 240, Total score this episode: -0.004999999888241291, Last 100 average: 0.048000000901520255\n",
      "Saved checkpoint with average score 0.061000001095235346\n",
      "Episode 260, Total score this episode: 0.8950000135228038, Last 100 average: 0.061000001095235346\n",
      "Saved checkpoint with average score 0.10750000178813934\n",
      "Episode 280, Total score this episode: -0.004999999888241291, Last 100 average: 0.10750000178813934\n",
      "Saved checkpoint with average score 0.10850000180304051\n",
      "Episode 300, Total score this episode: 0.04500000085681677, Last 100 average: 0.10850000180304051\n",
      "Saved checkpoint with average score 0.12300000201910734\n",
      "Episode 320, Total score this episode: 0.04500000085681677, Last 100 average: 0.12300000201910734\n",
      "Saved checkpoint with average score 0.17645000281743706\n",
      "Episode 340, Total score this episode: 0.04500000085681677, Last 100 average: 0.17645000281743706\n",
      "Saved checkpoint with average score 0.24895000389777125\n",
      "Episode 360, Total score this episode: -0.004999999888241291, Last 100 average: 0.24895000389777125\n",
      "Episode 380, Total score this episode: 0.245000003837049, Last 100 average: 0.2244500035326928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with average score 0.260500004068017\n",
      "Episode 400, Total score this episode: 0.9450000142678618, Last 100 average: 0.260500004068017\n",
      "Saved checkpoint with average score 0.296500004613772\n",
      "Episode 420, Total score this episode: 0.04500000085681677, Last 100 average: 0.296500004613772\n",
      "Episode 440, Total score this episode: 0.9950000150129199, Last 100 average: 0.2745500042848289\n",
      "Episode 460, Total score this episode: 0.19500000309199095, Last 100 average: 0.2075500032864511\n",
      "Episode 480, Total score this episode: 0.04500000085681677, Last 100 average: 0.266600004164502\n",
      "Episode 500, Total score this episode: 0.04500000178813934, Last 100 average: 0.2285500036086887\n",
      "Episode 520, Total score this episode: 0.04500000085681677, Last 100 average: 0.23210000365041197\n",
      "Early stopping. Best average score so far: 0.296500004613772\n",
      "Saving scores to ./ckpt/testing_05/scores.npy\n"
     ]
    }
   ],
   "source": [
    "# Training Hyperparameters\n",
    "EPISODES = 10000        # Number of episodes to train for\n",
    "# MAX_T = 2048          # Max length of trajectory\n",
    "MAX_T = 1000            # Max length of trajectory\n",
    "SGD_EPOCHS = 4          # Number of gradient descent steps per batch of experiences\n",
    "BATCH_SIZE = 32         # minibatch size\n",
    "BETA = 0.01             # entropy regularization parameter\n",
    "GRADIENT_CLIP = 5       # gradient clipping parameter\n",
    "\n",
    "# optimizer parameters\n",
    "# LR = 5e-4               # learning rate\n",
    "LR = 3e-4               # learning rate\n",
    "OP_EPSILON = 1e-5       # optimizer epsilon\n",
    "WEIGHT_DECAY = 1.E-4    # L2 weight decay\n",
    "\n",
    "# PPO parameters\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 0.95              # GAE parameter\n",
    "# PPO_CLIP_EPSILON = 0.1  # ppo clip parameter\n",
    "PPO_CLIP_EPSILON = 0.2  # ppo clip parameter\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import time\n",
    "env = UnityEnvironment(file_name=\"../unity_ml_envs/Tennis_Windows_x86_64/Tennis.exe\")\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "time.sleep(2)\n",
    "\n",
    "# Environment variables\n",
    "num_agents = len(env_info.agents)\n",
    "state_size = env_info.vector_observations.shape[1]\n",
    "action_size = brain.vector_action_space_size\n",
    "print(f\"Number of agents: {num_agents}. State size: {state_size}. Action size: {action_size}\")\n",
    "# Instantiate the agent\n",
    "agent = Agent(num_agents, state_size, action_size,\n",
    "                LR=LR,\n",
    "                op_epsilon=OP_EPSILON,\n",
    "                weight_decay=WEIGHT_DECAY,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                sgd_epochs=SGD_EPOCHS,\n",
    "                gradient_clip=GRADIENT_CLIP,\n",
    "                std=0.0,\n",
    "                value_size=1,\n",
    "                hidden_size=64,\n",
    "                clip_epsilon=PPO_CLIP_EPSILON,\n",
    "                c1=0.5,\n",
    "                beta=BETA)\n",
    "\n",
    "# Train the agent\n",
    "print(f\"Starting training with parameters LR={LR}, WEIGHT_DECAY={WEIGHT_DECAY}, BATCH_SIZE={BATCH_SIZE}, SGD_EPOCHS={SGD_EPOCHS}, GRADIENT_CLIP={GRADIENT_CLIP}, BETA={BETA}, GAMMA={GAMMA}, TAU={TAU}, PPO_CLIP_EPSILON={PPO_CLIP_EPSILON}\")\n",
    "# exit()\n",
    "train(env, brain_name, agent, num_agents, EPISODES, MAX_T,\n",
    "        gamma=GAMMA, tau=TAU, run_name=\"testing_05\", save_path=\".\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2. State size: 24. Action size: 2\n",
      "Starting training with parameters LR=0.0003, WEIGHT_DECAY=0.0001, BATCH_SIZE=32, SGD_EPOCHS=4, GRADIENT_CLIP=5, BETA=0.01, GAMMA=0.99, TAU=0.95, PPO_CLIP_EPSILON=0.1\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint with average score 1.8626451492309571e-10\n",
      "Episode 20, Total score this episode: -0.004999999888241291, Last 20 average: 1.8626451492309571e-10\n",
      "Saved checkpoint with average score 0.010000000358559191\n",
      "Episode 40, Total score this episode: -0.004999999888241291, Last 40 average: 0.010000000358559191\n",
      "Episode 60, Total score this episode: 0.04500000085681677, Last 60 average: 0.01000000035079817\n",
      "Saved checkpoint with average score 0.025625000579748303\n",
      "Episode 80, Total score this episode: 0.44500000681728125, Last 80 average: 0.025625000579748303\n",
      "Saved checkpoint with average score 0.03800000076182187\n",
      "Episode 100, Total score this episode: -0.004999999888241291, Last 100 average: 0.03800000076182187\n",
      "Saved checkpoint with average score 0.06950000123120845\n",
      "Episode 120, Total score this episode: 0.04500000085681677, Last 100 average: 0.06950000123120845\n",
      "Saved checkpoint with average score 0.1340000021830201\n",
      "Episode 140, Total score this episode: -0.004999999888241291, Last 100 average: 0.1340000021830201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5000828f5811>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# exit()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m train(env, brain_name, agent, num_agents, EPISODES, MAX_T,\n\u001b[1;32m---> 56\u001b[1;33m         gamma=GAMMA, tau=TAU, run_name=\"testing_05\", save_path=\".\")\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-78b770eadae7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, brain_name, agent, num_agents, n_episodes, max_t, gamma, tau, run_name, save_path)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_advantages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# print(f\"States: {states.shape}. Actions: {actions.shape}. Log_probs_old: {log_probs_old.shape}. Returns: {returns.shape}. Advantages: {advantages.shape}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mtest_mean_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Udacity\\DeepReinforcementLearningExpert\\udacity_reinforcement_project_collaboration_and_competition\\src\\agent.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, states, actions, log_probs_old, returns, advantages, sgd_epochs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 L = ppo_loss(self.model, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages,\n\u001b[1;32m--> 118\u001b[1;33m                              clip_epsilon=self.clip_epsilon, c1=self.c1, beta=self.beta)\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lucas\\Documents\\Udacity\\DeepReinforcementLearningExpert\\udacity_reinforcement_project_collaboration_and_competition\\src\\PPO.py\u001b[0m in \u001b[0;36mppo_loss\u001b[1;34m(model, states, actions, log_probs_old, returns, advantages, clip_epsilon, c1, beta)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# L_VF(θ) = ( V(s) - V_t )^2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mvalue_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreturns\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Hyperparameters\n",
    "EPISODES = 10000        # Number of episodes to train for\n",
    "# MAX_T = 2048          # Max length of trajectory\n",
    "MAX_T = 1000            # Max length of trajectory\n",
    "SGD_EPOCHS = 4          # Number of gradient descent steps per batch of experiences\n",
    "BATCH_SIZE = 32         # minibatch size\n",
    "BETA = 0.01             # entropy regularization parameter\n",
    "GRADIENT_CLIP = 5       # gradient clipping parameter\n",
    "\n",
    "# optimizer parameters\n",
    "# LR = 5e-4               # learning rate\n",
    "LR = 3e-4               # learning rate\n",
    "OP_EPSILON = 1e-5       # optimizer epsilon\n",
    "WEIGHT_DECAY = 1.E-4    # L2 weight decay\n",
    "\n",
    "# PPO parameters\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 0.95              # GAE parameter\n",
    "PPO_CLIP_EPSILON = 0.1  # ppo clip parameter\n",
    "# PPO_CLIP_EPSILON = 0.2  # ppo clip parameter\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import time\n",
    "env = UnityEnvironment(file_name=\"../unity_ml_envs/Tennis_Windows_x86_64/Tennis.exe\")\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "time.sleep(2)\n",
    "\n",
    "# Environment variables\n",
    "num_agents = len(env_info.agents)\n",
    "state_size = env_info.vector_observations.shape[1]\n",
    "action_size = brain.vector_action_space_size\n",
    "print(f\"Number of agents: {num_agents}. State size: {state_size}. Action size: {action_size}\")\n",
    "# Instantiate the agent\n",
    "agent = Agent(num_agents, state_size, action_size,\n",
    "                LR=LR,\n",
    "                op_epsilon=OP_EPSILON,\n",
    "                weight_decay=WEIGHT_DECAY,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                sgd_epochs=SGD_EPOCHS,\n",
    "                gradient_clip=GRADIENT_CLIP,\n",
    "                std=0.0,\n",
    "                value_size=1,\n",
    "                hidden_size=64,\n",
    "                clip_epsilon=PPO_CLIP_EPSILON,\n",
    "                c1=0.5,\n",
    "                beta=BETA)\n",
    "\n",
    "# Train the agent\n",
    "print(f\"Starting training with parameters LR={LR}, WEIGHT_DECAY={WEIGHT_DECAY}, BATCH_SIZE={BATCH_SIZE}, SGD_EPOCHS={SGD_EPOCHS}, GRADIENT_CLIP={GRADIENT_CLIP}, BETA={BETA}, GAMMA={GAMMA}, TAU={TAU}, PPO_CLIP_EPSILON={PPO_CLIP_EPSILON}\")\n",
    "# exit()\n",
    "train(env, brain_name, agent, num_agents, EPISODES, MAX_T,\n",
    "        gamma=GAMMA, tau=TAU, run_name=\"testing_05\", save_path=\".\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "- Plot rewards\n",
    "- Hyperparameters influence\n",
    "\n",
    "## Observations\n",
    "\n",
    "- Effect of MAX_T on the performance\n",
    "\n",
    "\n",
    "## Ideas for future work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
